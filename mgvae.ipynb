{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mgvae.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMI/vdh3CmIE2YuED28c9An",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachnguyenTE/temporal-mgn/blob/prototype-mgvae/mgvae.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuoLHm2oa_zC",
        "outputId": "66e07c08-68d4-48be-dbfb-f99a2118e4e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Apr  6 08:53:30 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments (inactive)\n",
        "\n",
        "# def _parse_args():\n",
        "#     parser = argparse.ArgumentParser(description = 'Temporal graph learning')\n",
        "#     parser.add_argument('--dir', '-dir', type = str, default = '.', help = 'Directory')\n",
        "#     parser.add_argument('--learning_target', '-learning_target', type = str, default = 'U0', help = 'Learning target')\n",
        "#     parser.add_argument('--name', '-name', type = str, default = 'NAME', help = 'Name')\n",
        "#     parser.add_argument('--dataset', '-dataset', type = str, default = 'ZINC_12k', help = 'ZINC')\n",
        "#     parser.add_argument('--num_epoch', '-num_epoch', type = int, default = 2048, help = 'Number of epochs')\n",
        "#     parser.add_argument('--batch_size', '-batch_size', type = int, default = 20, help = 'Batch size')\n",
        "#     parser.add_argument('--learning_rate', '-learning_rate', type = float, default = 0.001, help = 'Initial learning rate')\n",
        "#     parser.add_argument('--seed', '-s', type = int, default = 123456789, help = 'Random seed')\n",
        "#     parser.add_argument('--n_clusters', '-n_clusters', type = int, default = 2, help = 'Number of clusters')\n",
        "#     parser.add_argument('--n_levels', '-n_levels', type = int, default = 3, help = 'Number of levels of resolution')\n",
        "#     parser.add_argument('--n_layers', '-n_layers', type = int, default = 3, help = 'Number of layers of message passing')\n",
        "#     parser.add_argument('--hidden_dim', '-hidden_dim', type = int, default = 32, help = 'Hidden dimension')\n",
        "#     parser.add_argument('--z_dim', '-z_dim', type = int, default = 32, help = 'Latent dimension')\n",
        "#     parser.add_argument('--device', '-device', type = str, default = 'cpu', help = 'cuda/cpu')\n",
        "#     args = parser.parse_args()\n",
        "#     return args\n",
        "\n",
        "# args = _parse_args()\n",
        "# log_name = args.dir + \"/\" + args.name + \".log\"\n",
        "# model_name = args.dir + \"/\" + args.name + \".model\"\n",
        "# LOG = open(log_name, \"w\")"
      ],
      "metadata": {
        "id": "p70wMn16dXq-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "%%capture\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric "
      ],
      "metadata": {
        "id": "izcOBzw6bCDv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library import (legacy MGVAE)\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, Adagrad\n",
        "import pickle\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "# Library import (pytorch-geometric)\n",
        "import torch_geometric \n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import DataLoader, ClusterLoader, ClusterData\n",
        "from torch_geometric.nn import MessagePassing, GCNConv\n",
        "\n",
        "###############################################################\n",
        "# NOTE: \n",
        "# We preferably define our own clustering \n",
        "# procedure, rather than using the built-in ClusterLoader\n",
        "# since there is a chance using ClusterLoader will not\n",
        "# make the entire net differentiable (separate data process),\n",
        "# and the net may no longer be isomorphic invariant.\n",
        "###############################################################"
      ],
      "metadata": {
        "id": "4Bw6mX9dbcwf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix all random seed\n",
        "torch_geometric.seed.seed_everything(69420)\n",
        "\n",
        "# Set device to gpu\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "PjiJCDQ5h_ww"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define glorot initialization\n",
        "def glorot_init(input_dim, output_dim):\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = torch.rand(input_dim, output_dim) * 2 * init_range - init_range\n",
        "    return nn.Parameter(initial)"
      ],
      "metadata": {
        "id": "ERrvyJPjJug1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiresolution Graph Network\n",
        "class MGN(nn.Module):\n",
        "    def __init__(self, clusters, num_layers, node_dim, edge_dim, hidden_dim, z_dim):\n",
        "        super(MGN, self).__init__()\n",
        "        self.clusters = clusters\n",
        "        self.num_layers = num_layers\n",
        "        self.node_dim = node_dim\n",
        "        self.edge_dim = edge_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.base_encoder = GraphEncoder(self.num_layers, self.node_dim, self.edge_dim, self.hidden_dim, self.z_dim)\n",
        "\n",
        "        self.cluster_learner = nn.ModuleList()\n",
        "        self.global_encoder = nn.ModuleList()\n",
        "        for i in range(len(self.clusters)):\n",
        "            N = self.clusters[i]\n",
        "            self.cluster_learner.append(GraphCluster(self.num_layers, self.z_dim, self.hidden_dim, N))\n",
        "            self.global_encoder.append(GraphEncoder(self.num_layers, self.z_dim, None, self.hidden_dim, self.z_dim))\n",
        "\n",
        "    def forward(self, adj, node_feat, edge_feat = None):\n",
        "        outputs = []\n",
        "\n",
        "        # Base encoder\n",
        "        base_latent = self.base_encoder(adj, node_feat, edge_feat)\n",
        "\n",
        "        outputs.append([base_latent, adj])\n",
        "\n",
        "        l = len(self.clusters) - 1\n",
        "        while l >= 0:\n",
        "            if l == len(self.clusters) - 1:\n",
        "                prev_adj = adj\n",
        "                prev_latent = base_latent\n",
        "            else:\n",
        "                prev_adj = outputs[len(outputs) - 1][1]\n",
        "                prev_latent = outputs[len(outputs) - 1][0]\n",
        "\n",
        "            # Assignment score\n",
        "            assign_score = self.cluster_learner[l](prev_adj, prev_latent)\n",
        "\n",
        "            # Softmax (soft assignment)\n",
        "            # assign_matrix = F.softmax(assign_score, dim = 2)\n",
        "\n",
        "            # Gumbel softmax (hard assignment)\n",
        "            assign_matrix = F.gumbel_softmax(assign_score, tau = 1, hard = True, dim = 2)\n",
        "\n",
        "            # Print out the cluster assignment matrix\n",
        "            # print(torch.sum(assign_matrix, dim = 0))\n",
        "\n",
        "            # Shrinked latent\n",
        "            shrinked_latent = torch.matmul(assign_matrix.transpose(1, 2), prev_latent)\n",
        "\n",
        "            # Latent normalization\n",
        "            shrinked_latent = F.normalize(shrinked_latent, dim = 1)\n",
        "\n",
        "            # Shrinked adjacency\n",
        "            shrinked_adj = torch.matmul(torch.matmul(assign_matrix.transpose(1, 2), prev_adj), assign_matrix)\n",
        "\n",
        "            # Adjacency normalization\n",
        "            shrinked_adj = shrinked_adj / torch.sum(shrinked_adj)\n",
        "\n",
        "            # Global encoder\n",
        "            next_latent = self.global_encoder[l](shrinked_adj, shrinked_latent)\n",
        "\n",
        "            outputs.append([next_latent, shrinked_adj])\n",
        "            l -= 1\n",
        "\n",
        "        # Scalar prediction\n",
        "        latent = torch.cat([torch.sum(output[0], dim = 1) for output in outputs], dim = 1)\n",
        "        hidden = torch.tanh(self.fc1(latent))\n",
        "        predict = self.fc2(hidden)\n",
        "\n",
        "        return predict, latent, outputs"
      ],
      "metadata": {
        "id": "0unZS5oei86U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph encoder block\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, node_dim, edge_dim, hidden_dim, z_dim, use_concat_layer=True, **kwargs):\n",
        "        super(GraphEncoder, self).__init__(**kwargs)\n",
        "        self.clusters = clusters\n",
        "        self.num_layers = num_layers\n",
        "        self.node_dim = node_dim\n",
        "        self.edge_dim = edge_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.use_concat_layer = use_concat_layer\n",
        "\n",
        "        self.node_fc1 = nn.Linear(self.node_dim, 128)\n",
        "        self.node_fc2 = nn.Linear(128, self.hidden_dim)\n",
        "\n",
        "        if self.edge_dim is not None:\n",
        "            self.edge_fc1 = nn.Linear(self.edge_dim, 128)\n",
        "            self.edge_fc2 = nn.Linear(128, self.hidden_dim)\n",
        "\n",
        "        self.base_net = nn.ModuleList()\n",
        "        self.combine_net = nn.ModuleList()\n",
        "        for layer in range(self.num_layers):\n",
        "            self.base_net.append(GCNConv(self.hidden_dim, self.hidden_dim))\n",
        "            if self.edge_dim is not None:\n",
        "                self.combine_net.append(nn.Linear(2 * self.hidden_dim, self.hidden_dim))\n",
        "\n",
        "        if self.use_concat_layer == True:\n",
        "            self.latent_fc1 = nn.Linear((self.num_layers + 1) * self.hidden_dim, 256)\n",
        "            self.latent_fc2 = nn.Linear(256, self.z_dim)\n",
        "        else:\n",
        "            self.latent_fc1 = nn.Linear(self.hidden_dim, 256)\n",
        "            self.latent_fc2 = nn.Linear(256, self.z_dim)\n",
        "\n",
        "    def forward(self, adj, node_feat, edge_feat=None):\n",
        "        node_hidden = torch.tanh(self.node_fc1(node_feat))\n",
        "        node_hidden = torch.tanh(self.node_fc2(node_hidden))\n",
        "\n",
        "        if edge_feat is not None and self.edge_dim is not None:\n",
        "            edge_hidden = torch.tanh(self.edge_fc1(edge_feat))\n",
        "            edge_hidden = torch.tanh(self.edge_fc2(edge_hidden))\n",
        "\n",
        "        all_hidden = [node_hidden]\n",
        "        for layer in range(len(self.base_net)):\n",
        "            if layer == 0:\n",
        "                hidden = self.base_net[layer](adj, node_hidden)\n",
        "            else:\n",
        "                hidden = self.base_net[layer](adj, hidden)\n",
        "            \n",
        "            if edge_feat is not None and self.edge_dim is not None:\n",
        "                hidden = torch.cat((hidden, torch.tanh(torch.einsum('bijc,bjk->bik', edge_hidden, hidden))), dim = 2)\n",
        "                hidden = torch.tanh(self.combine_net[layer](hidden))\n",
        "        \n",
        "            all_hidden.append(hidden)\n",
        "\n",
        "        if self.use_concat_layer == True:\n",
        "            hidden = torch.cat(all_hidden, dim = 2)\n",
        "\n",
        "        latent = torch.tanh(self.latent_fc1(hidden))\n",
        "        latent = torch.tanh(self.latent_fc2(latent))\n",
        "        return latent"
      ],
      "metadata": {
        "id": "PK0GXjEK_zqq"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph clustering block\n",
        "class GraphCluster(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, z_dim, **kwargs):\n",
        "        super(GraphCluster, self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, self.hidden_dim)\n",
        "\n",
        "        # Option 1: Learnable clustering\n",
        "        # self.base_net = nn.ModuleList()\n",
        "        \n",
        "        # Option 2: Fixed clustering\n",
        "        self.base_net = []\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            self.base_net.append(GCNConv(self.hidden_dim, self.hidden_dim))\n",
        "\n",
        "        self.assign_net = GCNConv(self.hidden_dim, self.z_dim)\n",
        "\n",
        "    def forward(self, adj, X):\n",
        "        hidden = torch.sigmoid(self.fc1(X))\n",
        "        hidden = torch.sigmoid(self.fc2(hidden))\n",
        "        for net in self.base_net:\n",
        "            hidden = net(adj, hidden)\n",
        "        assign = self.assign_net(adj, hidden)\n",
        "        return assign"
      ],
      "metadata": {
        "id": "zr-QR7HkGfNh"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "vKYTHa50JMBG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}