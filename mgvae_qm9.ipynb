{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mgvae.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOzXjDFUszFbdnJJuVDW4iM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bachnguyenTE/temporal-mgn/blob/prototype-mgvae/mgvae_qm9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuoLHm2oa_zC",
        "outputId": "863346bd-011f-4cd1-b9e8-d0f4fbae0712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 19 13:57:56 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   32C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Arguments (inactive)\n",
        "\n",
        "# def _parse_args():\n",
        "#     parser = argparse.ArgumentParser(description = 'Temporal graph learning')\n",
        "#     parser.add_argument('--dir', '-dir', type = str, default = '.', help = 'Directory')\n",
        "#     parser.add_argument('--learning_target', '-learning_target', type = str, default = 'U0', help = 'Learning target')\n",
        "#     parser.add_argument('--name', '-name', type = str, default = 'NAME', help = 'Name')\n",
        "#     parser.add_argument('--dataset', '-dataset', type = str, default = 'ZINC_12k', help = 'ZINC')\n",
        "#     parser.add_argument('--num_epoch', '-num_epoch', type = int, default = 2048, help = 'Number of epochs')\n",
        "#     parser.add_argument('--batch_size', '-batch_size', type = int, default = 20, help = 'Batch size')\n",
        "#     parser.add_argument('--learning_rate', '-learning_rate', type = float, default = 0.001, help = 'Initial learning rate')\n",
        "#     parser.add_argument('--seed', '-s', type = int, default = 123456789, help = 'Random seed')\n",
        "#     parser.add_argument('--n_clusters', '-n_clusters', type = int, default = 2, help = 'Number of clusters')\n",
        "#     parser.add_argument('--n_levels', '-n_levels', type = int, default = 3, help = 'Number of levels of resolution')\n",
        "#     parser.add_argument('--n_layers', '-n_layers', type = int, default = 3, help = 'Number of layers of message passing')\n",
        "#     parser.add_argument('--hidden_dim', '-hidden_dim', type = int, default = 32, help = 'Hidden dimension')\n",
        "#     parser.add_argument('--z_dim', '-z_dim', type = int, default = 32, help = 'Latent dimension')\n",
        "#     parser.add_argument('--device', '-device', type = str, default = 'cpu', help = 'cuda/cpu')\n",
        "#     args = parser.parse_args()\n",
        "#     return args\n",
        "\n",
        "# args = _parse_args()\n",
        "# log_name = args.dir + \"/\" + args.name + \".log\"\n",
        "# model_name = args.dir + \"/\" + args.name + \".model\"\n",
        "# LOG = open(log_name, \"w\")"
      ],
      "metadata": {
        "id": "p70wMn16dXq-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add this in a Google Colab cell to install the correct version of Pytorch Geometric.\n",
        "%%capture\n",
        "import torch\n",
        "\n",
        "def format_pytorch_version(version):\n",
        "  return version.split('+')[0]\n",
        "\n",
        "TORCH_version = torch.__version__\n",
        "TORCH = format_pytorch_version(TORCH_version)\n",
        "\n",
        "def format_cuda_version(version):\n",
        "  return 'cu' + version.replace('.', '')\n",
        "\n",
        "CUDA_version = torch.version.cuda\n",
        "CUDA = format_cuda_version(CUDA_version)\n",
        "\n",
        "!pip install torch-scatter     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-sparse      -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-cluster     -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-spline-conv -f https://pytorch-geometric.com/whl/torch-{TORCH}+{CUDA}.html\n",
        "!pip install torch-geometric \n",
        "\n",
        "!pip install einops\n",
        "!wget -c https://gist.githubusercontent.com/Luvata/55f7b3e9ae451122b9e3faf0a7387b4f/raw/440fac5c6e7153fd39e4eb9ebec6e51c9520ef1f/visualize.py\n",
        "!pip install --upgrade graphviz"
      ],
      "metadata": {
        "id": "izcOBzw6bCDv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Library import (legacy MGVAE)\n",
        "import argparse\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import Adam, Adagrad\n",
        "import pickle\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import argparse\n",
        "\n",
        "# Library import (pytorch-geometric)\n",
        "import torch_geometric \n",
        "import torch_geometric.transforms as T\n",
        "from torch_geometric.loader import DataLoader, ClusterLoader, ClusterData\n",
        "from torch_geometric.nn import MessagePassing, GCNConv\n",
        "from torch_geometric.utils import to_dense_adj, dense_to_sparse\n",
        "\n",
        "###############################################################\n",
        "# NOTE: \n",
        "# We preferably define our own clustering \n",
        "# procedure, rather than using the built-in ClusterLoader\n",
        "# since there is a chance using ClusterLoader will not\n",
        "# make the entire net differentiable (separate data process),\n",
        "# and the net may no longer be isomorphic invariant.\n",
        "###############################################################\n",
        "\n",
        "from visualize import display_module"
      ],
      "metadata": {
        "id": "4Bw6mX9dbcwf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fix all random seed\n",
        "torch_geometric.seed.seed_everything(69420)\n",
        "\n",
        "# Set device to gpu\n",
        "device = torch.device('cuda')"
      ],
      "metadata": {
        "id": "PjiJCDQ5h_ww"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define glorot initialization\n",
        "def glorot_init(input_dim, output_dim):\n",
        "    init_range = np.sqrt(6.0 / (input_dim + output_dim))\n",
        "    initial = torch.rand(input_dim, output_dim) * 2 * init_range - init_range\n",
        "    return nn.Parameter(initial)"
      ],
      "metadata": {
        "id": "ERrvyJPjJug1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Multiresolution Graph Network\n",
        "class MGN(nn.Module):\n",
        "    def __init__(self, clusters, num_layers, node_dim, edge_dim, hidden_dim, z_dim, num_classes):\n",
        "        super(MGN, self).__init__()\n",
        "        self.clusters = clusters\n",
        "        self.num_layers = num_layers\n",
        "        self.node_dim = node_dim\n",
        "        self.edge_dim = edge_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.num_classes = num_classes\n",
        "\n",
        "        self.base_encoder = GraphEncoder(self.num_layers, self.node_dim, self.edge_dim, self.hidden_dim, self.z_dim)\n",
        "\n",
        "        self.cluster_learner = nn.ModuleList()\n",
        "        self.global_encoder = nn.ModuleList()\n",
        "        for i in range(len(self.clusters)):\n",
        "            N = self.clusters[i]\n",
        "            self.cluster_learner.append(GraphCluster(self.num_layers, self.z_dim, self.hidden_dim, N))\n",
        "            self.global_encoder.append(GraphEncoder(self.num_layers, self.z_dim, None, self.hidden_dim, self.z_dim))\n",
        "\n",
        "        D = self.z_dim * (len(self.clusters) + 1)\n",
        "        self.fc1 = nn.Linear(D, 256)\n",
        "        self.fc2 = nn.Linear(256, num_classes)\n",
        "\n",
        "    def forward(self, adj, node_feat, edge_feat = None):\n",
        "        outputs = []\n",
        "\n",
        "        # Base encoder\n",
        "        base_latent = self.base_encoder(adj, node_feat, edge_feat)\n",
        "\n",
        "        outputs.append([base_latent, adj])\n",
        "\n",
        "        l = 0\n",
        "        while l < len(self.clusters):\n",
        "            if l == 0:\n",
        "                prev_adj = adj\n",
        "                prev_latent = base_latent\n",
        "            else:\n",
        "                prev_adj = outputs[len(outputs) - 1][1]\n",
        "                prev_latent = outputs[len(outputs) - 1][0]\n",
        "\n",
        "            # Assignment score\n",
        "            assign_score = self.cluster_learner[l](prev_adj, prev_latent)\n",
        "\n",
        "            # Softmax (soft assignment)\n",
        "            # assign_matrix = F.softmax(assign_score, dim = 2)\n",
        "\n",
        "            # Gumbel softmax (hard assignment)\n",
        "            assign_matrix = F.gumbel_softmax(assign_score, tau = 1, hard = True, dim = 1)\n",
        "\n",
        "            # Print out the cluster assignment matrix\n",
        "            # print(torch.sum(assign_matrix, dim = 0))\n",
        "\n",
        "            # Shrinked latent\n",
        "            shrinked_latent = torch.matmul(assign_matrix.transpose(0, 1), prev_latent)\n",
        "\n",
        "            # Latent normalization\n",
        "            shrinked_latent = F.normalize(shrinked_latent, dim = 0)\n",
        "\n",
        "            # Shrinked adjacency\n",
        "            # print(f'Iteration: {l}')\n",
        "            # print(to_dense_adj(prev_adj))\n",
        "            # print(f'to_dense_adj size: {to_dense_adj(prev_adj, max_num_nodes=self.clusters[l]).size()}')\n",
        "            # print(f'assign_matrix size: {assign_matrix.size()}')\n",
        "            # print(f'node_feat size: {node_feat.size()}')\n",
        "            # print(f'adj size: {prev_adj.size()}')\n",
        "            # print(f'prev_adj size: {prev_adj.size()}')\n",
        "            if l == 0:\n",
        "                shrinked_adj = torch.matmul(torch.matmul(assign_matrix.transpose(0, 1), to_dense_adj(prev_adj, max_num_nodes=node_feat.size()[0])[0]), assign_matrix)\n",
        "            else:\n",
        "                shrinked_adj = torch.matmul(torch.matmul(assign_matrix.transpose(0, 1), to_dense_adj(prev_adj, max_num_nodes=self.clusters[l - 1])[0]), assign_matrix)\n",
        "\n",
        "            # Adjacency normalization\n",
        "            shrinked_adj = shrinked_adj / torch.sum(shrinked_adj)\n",
        "\n",
        "            # Reformatting adjacency matrix as edge index\n",
        "            shrinked_adj = shrinked_adj[None, :]\n",
        "            # print(f'shrinked_adj matrix: {shrinked_adj}')\n",
        "            shrinked_adj, _ = dense_to_sparse(shrinked_adj)\n",
        "\n",
        "            # Global encoder\n",
        "            next_latent = self.global_encoder[l](shrinked_adj, shrinked_latent)\n",
        "\n",
        "            outputs.append([next_latent, shrinked_adj])\n",
        "            l += 1\n",
        "\n",
        "        # Scalar prediction\n",
        "        # print(f'size of output elem: {outputs[0][0].size()}')\n",
        "        latent = torch.cat([torch.sum(output[0], dim = 0) for output in outputs], dim = 0)\n",
        "        # print(f'final latent size: {latent.size()}')\n",
        "        hidden = torch.tanh(self.fc1(latent))\n",
        "        predict = self.fc2(hidden)\n",
        "\n",
        "        return predict, latent, outputs"
      ],
      "metadata": {
        "id": "0unZS5oei86U"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph encoder block\n",
        "class GraphEncoder(nn.Module):\n",
        "    def __init__(self, num_layers, node_dim, edge_dim, hidden_dim, z_dim, use_concat_layer=True, **kwargs):\n",
        "        super(GraphEncoder, self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "        self.node_dim = node_dim\n",
        "        self.edge_dim = edge_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.z_dim = z_dim\n",
        "        self.use_concat_layer = use_concat_layer\n",
        "\n",
        "        self.node_fc1 = nn.Linear(self.node_dim, 128)\n",
        "        self.node_fc2 = nn.Linear(128, self.hidden_dim)\n",
        "\n",
        "        if self.edge_dim is not None:\n",
        "            self.edge_fc1 = nn.Linear(self.edge_dim, 128)\n",
        "            self.edge_fc2 = nn.Linear(128, self.hidden_dim)\n",
        "\n",
        "        self.base_net = nn.ModuleList()\n",
        "        self.combine_net = nn.ModuleList()\n",
        "        for layer in range(self.num_layers):\n",
        "            self.base_net.append(GCNConv(self.hidden_dim, self.hidden_dim))\n",
        "            if self.edge_dim is not None:\n",
        "                self.combine_net.append(nn.Linear(2 * self.hidden_dim, self.hidden_dim))\n",
        "\n",
        "        if self.use_concat_layer == True:\n",
        "            self.latent_fc1 = nn.Linear((self.num_layers + 1) * self.hidden_dim, 256)\n",
        "            self.latent_fc2 = nn.Linear(256, self.z_dim)\n",
        "        else:\n",
        "            self.latent_fc1 = nn.Linear(self.hidden_dim, 256)\n",
        "            self.latent_fc2 = nn.Linear(256, self.z_dim)\n",
        "\n",
        "    def forward(self, adj, node_feat, edge_feat=None):\n",
        "        node_hidden = torch.tanh(self.node_fc1(node_feat))\n",
        "        node_hidden = torch.tanh(self.node_fc2(node_hidden))\n",
        "\n",
        "        if edge_feat is not None and self.edge_dim is not None:\n",
        "            edge_hidden = torch.tanh(self.edge_fc1(edge_feat))\n",
        "            edge_hidden = torch.tanh(self.edge_fc2(edge_hidden))\n",
        "\n",
        "        all_hidden = [node_hidden]\n",
        "        for layer in range(len(self.base_net)):\n",
        "            if layer == 0:\n",
        "                hidden = self.base_net[layer](node_hidden, adj)\n",
        "            else:\n",
        "                hidden = self.base_net[layer](node_hidden, adj)\n",
        "            \n",
        "            if edge_feat is not None and self.edge_dim is not None:\n",
        "                hidden = torch.cat((hidden, torch.tanh(torch.einsum('bijc,bjk->bik', edge_hidden, hidden))), dim = 2)\n",
        "                hidden = torch.tanh(self.combine_net[layer](hidden))\n",
        "        \n",
        "            all_hidden.append(hidden)\n",
        "\n",
        "        if self.use_concat_layer == True:\n",
        "            hidden = torch.cat(all_hidden, dim=1)\n",
        "\n",
        "        latent = torch.tanh(self.latent_fc1(hidden))\n",
        "        latent = torch.tanh(self.latent_fc2(latent))\n",
        "        return latent"
      ],
      "metadata": {
        "id": "PK0GXjEK_zqq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Graph clustering block\n",
        "class GraphCluster(nn.Module):\n",
        "    def __init__(self, num_layers, input_dim, hidden_dim, z_dim, **kwargs):\n",
        "        super(GraphCluster, self).__init__(**kwargs)\n",
        "        self.num_layers = num_layers\n",
        "        self.input_dim = input_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.fc1 = nn.Linear(self.input_dim, 128)\n",
        "        self.fc2 = nn.Linear(128, self.hidden_dim)\n",
        "\n",
        "        # Option 1: Learnable clustering\n",
        "        self.base_net = nn.ModuleList()\n",
        "        \n",
        "        # Option 2: Fixed clustering\n",
        "        # self.base_net = []\n",
        "\n",
        "        for layer in range(self.num_layers):\n",
        "            self.base_net.append(GCNConv(self.hidden_dim, self.hidden_dim))\n",
        "\n",
        "        self.assign_net = GCNConv(self.hidden_dim, self.z_dim)\n",
        "\n",
        "    def forward(self, adj, X):\n",
        "        hidden = torch.sigmoid(self.fc1(X))\n",
        "        hidden = torch.sigmoid(self.fc2(hidden))\n",
        "        for net in self.base_net:\n",
        "            hidden = net(hidden, adj)\n",
        "        assign = self.assign_net(hidden, adj)\n",
        "        return assign"
      ],
      "metadata": {
        "id": "zr-QR7HkGfNh"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset testing\n",
        "from torch_geometric.datasets import ZINC, QM9, TUDataset, KarateClub, GNNBenchmarkDataset\n",
        "\n",
        "# dataset = ZINC('data/ZINC')\n",
        "dataset = QM9(root='data/QM7b')\n",
        "# dataset = TUDataset(root='data/TUDataset', name='PROTEINS')\n",
        "# dataset = KarateClub()\n",
        "# dataset = GNNBenchmarkDataset(root='data/GNNBenchmarkDataset', name='MNIST')\n",
        "\n",
        "print(f'Dataset: {dataset}:')\n",
        "print('====================')\n",
        "print(f'Number of graphs: {len(dataset)}')\n",
        "print(f'Number of features: {dataset.num_features}')\n",
        "print(f'Number of classes: {dataset.num_classes}')\n",
        "\n",
        "data = dataset[0] # Get the first graph object\n",
        "\n",
        "print()\n",
        "print(data)\n",
        "print('=============================================================')\n",
        "\n",
        "# Gather some statistics about the first graph\n",
        "print(f'Number of nodes: {data.num_nodes}')\n",
        "print(f'Number of edges: {data.num_edges}')\n",
        "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
        "print(f'Has isolated nodes: {data.has_isolated_nodes()}')\n",
        "print(f'Has self-loops: {data.has_self_loops()}')\n",
        "print(f'Is undirected: {data.is_undirected()}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npW58Keh2bNs",
        "outputId": "a3b1a0d4-fd25-4c5c-c336-43292dcc2d46"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: QM9(130831):\n",
            "====================\n",
            "Number of graphs: 130831\n",
            "Number of features: 11\n",
            "Number of classes: 19\n",
            "\n",
            "Data(x=[5, 11], edge_index=[2, 8], edge_attr=[8, 4], y=[1, 19], pos=[5, 3], idx=[1], name='gdb_1', z=[5])\n",
            "=============================================================\n",
            "Number of nodes: 5\n",
            "Number of edges: 8\n",
            "Average node degree: 1.60\n",
            "Has isolated nodes: False\n",
            "Has self-loops: False\n",
            "Is undirected: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.shuffle()\n",
        "\n",
        "train_dataset = dataset[:1000]\n",
        "test_dataset = dataset[1000:1100]\n",
        "\n",
        "print(f'Number of training graphs: {len(train_dataset)}')\n",
        "print(f'Number of test graphs: {len(test_dataset)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X3A-6D-K3w8O",
        "outputId": "c2240f1f-9d01-4159-ebe6-100cab49d431"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training graphs: 1000\n",
            "Number of test graphs: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Minibatching the dataset \n",
        "from torch_geometric.loader import DataLoader\n",
        "\n",
        "train_loader = DataLoader(train_dataset, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, shuffle=False)"
      ],
      "metadata": {
        "id": "zrWDOD1B39hd"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# adj is the adjacency matrix\n",
        "# PyG: need to convert graph data format to the adjacency matrix format, or rewrite code"
      ],
      "metadata": {
        "id": "4eTq69e6_3Mb"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Javascript\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 300})'''))\n",
        "\n",
        "model = MGN(\n",
        "    clusters=[4, 2],\n",
        "    num_layers=4,\n",
        "    node_dim=dataset.num_features,\n",
        "    edge_dim=None,\n",
        "    hidden_dim=64,\n",
        "    z_dim=16,\n",
        "    num_classes=dataset.num_classes\n",
        ").to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "criterion = torch.nn.MSELoss()\n",
        "\n",
        "def train():\n",
        "    model.train()\n",
        "\n",
        "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
        "        data.to(device)\n",
        "        predict, latent, outputs = model(data.edge_index, data.x)  # Perform a single forward pass.\n",
        "        loss = criterion(predict[None, :], data.y)  # Compute the loss.\n",
        "        loss.backward()  # Derive gradients.\n",
        "        optimizer.step()  # Update parameters based on gradients.\n",
        "        optimizer.zero_grad()  # Clear gradients.\n",
        "\n",
        "def test(loader):\n",
        "    model.eval()\n",
        "\n",
        "    correct = 0\n",
        "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        data.to(device)\n",
        "        predict, latent, outputs = model(data.edge_index, data.x)  \n",
        "        modelLoss = criterion(predict[None, :], data.y)\n",
        "    return modelLoss  # Derive ratio of correct predictions.\n",
        "\n",
        "\n",
        "for epoch in range(1, 171):\n",
        "    train()\n",
        "    train_acc = test(train_loader)\n",
        "    test_acc = test(test_loader)\n",
        "    print(f'Epoch: {epoch:03d}, Train Loss: {train_acc:.4f}, Test Loss: {test_acc:.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "xhM4wbg04PVT",
        "outputId": "049a2234-2250-44da-ac1c-db868150b35f"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 300})"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001, Train Loss: 10523363.0000, Test Loss: 31387712.0000\n",
            "Epoch: 002, Train Loss: 7632530.5000, Test Loss: 21199454.0000\n",
            "Epoch: 003, Train Loss: 5837754.0000, Test Loss: 13780696.0000\n",
            "Epoch: 004, Train Loss: 902384.4375, Test Loss: 8641111.0000\n",
            "Epoch: 005, Train Loss: 774752.1875, Test Loss: 5361662.5000\n",
            "Epoch: 006, Train Loss: 714150.8750, Test Loss: 3533513.2500\n",
            "Epoch: 007, Train Loss: 6508.6162, Test Loss: 2755543.0000\n",
            "Epoch: 008, Train Loss: 371119.2188, Test Loss: 2518667.0000\n",
            "Epoch: 009, Train Loss: 93806.3203, Test Loss: 2512439.5000\n",
            "Epoch: 010, Train Loss: 11583.3301, Test Loss: 2527837.7500\n",
            "Epoch: 011, Train Loss: 342976.7812, Test Loss: 2462667.2500\n",
            "Epoch: 012, Train Loss: 232957.3750, Test Loss: 2499491.5000\n",
            "Epoch: 013, Train Loss: 344506.0938, Test Loss: 2515739.5000\n",
            "Epoch: 014, Train Loss: 92312.4453, Test Loss: 2512872.0000\n",
            "Epoch: 015, Train Loss: 34557.6289, Test Loss: 2485619.0000\n",
            "Epoch: 016, Train Loss: 109966.9766, Test Loss: 2464449.7500\n",
            "Epoch: 017, Train Loss: 134070.1562, Test Loss: 2496127.5000\n",
            "Epoch: 018, Train Loss: 135055.7188, Test Loss: 2467531.5000\n",
            "Epoch: 019, Train Loss: 128362.8828, Test Loss: 2487898.5000\n",
            "Epoch: 020, Train Loss: 105782.6641, Test Loss: 2481332.0000\n",
            "Epoch: 021, Train Loss: 19347.6250, Test Loss: 2474993.2500\n",
            "Epoch: 022, Train Loss: 18726.9844, Test Loss: 2457553.2500\n",
            "Epoch: 023, Train Loss: 17709.1582, Test Loss: 2469957.0000\n",
            "Epoch: 024, Train Loss: 250198.0312, Test Loss: 2478900.7500\n",
            "Epoch: 025, Train Loss: 16459.1289, Test Loss: 2462608.7500\n",
            "Epoch: 026, Train Loss: 20999.0684, Test Loss: 2447488.7500\n",
            "Epoch: 027, Train Loss: 622889.8750, Test Loss: 2481115.7500\n",
            "Epoch: 028, Train Loss: 306216.5625, Test Loss: 2492791.0000\n",
            "Epoch: 029, Train Loss: 949302.3125, Test Loss: 2478290.7500\n",
            "Epoch: 030, Train Loss: 104708.5234, Test Loss: 2484160.7500\n",
            "Epoch: 031, Train Loss: 24407.2090, Test Loss: 2499484.7500\n",
            "Epoch: 032, Train Loss: 11634.9805, Test Loss: 2512211.0000\n",
            "Epoch: 033, Train Loss: 651652.8125, Test Loss: 2489948.7500\n",
            "Epoch: 034, Train Loss: 635047.7500, Test Loss: 2500706.7500\n",
            "Epoch: 035, Train Loss: 11730.7695, Test Loss: 2486058.0000\n",
            "Epoch: 036, Train Loss: 1512366.8750, Test Loss: 2523422.0000\n",
            "Epoch: 037, Train Loss: 126454.6875, Test Loss: 2467692.2500\n",
            "Epoch: 038, Train Loss: 10572.3955, Test Loss: 2492195.0000\n",
            "Epoch: 039, Train Loss: 596848.1875, Test Loss: 2474886.0000\n",
            "Epoch: 040, Train Loss: 231309.8125, Test Loss: 2505778.0000\n",
            "Epoch: 041, Train Loss: 430.2838, Test Loss: 2490490.5000\n",
            "Epoch: 042, Train Loss: 4533.9790, Test Loss: 2447078.0000\n",
            "Epoch: 043, Train Loss: 571516.0000, Test Loss: 2484353.5000\n",
            "Epoch: 044, Train Loss: 104517.6953, Test Loss: 2474033.7500\n",
            "Epoch: 045, Train Loss: 17903.2656, Test Loss: 2472179.0000\n",
            "Epoch: 046, Train Loss: 115801.5391, Test Loss: 2526675.0000\n",
            "Epoch: 047, Train Loss: 625616.6250, Test Loss: 2555993.2500\n",
            "Epoch: 048, Train Loss: 590508.4375, Test Loss: 2532193.2500\n",
            "Epoch: 049, Train Loss: 635204.0000, Test Loss: 2491959.7500\n",
            "Epoch: 050, Train Loss: 92506.3438, Test Loss: 2468708.7500\n",
            "Epoch: 051, Train Loss: 333835.3750, Test Loss: 2464249.0000\n",
            "Epoch: 052, Train Loss: 588.7433, Test Loss: 2503708.5000\n",
            "Epoch: 053, Train Loss: 1030410.8750, Test Loss: 2452679.5000\n",
            "Epoch: 054, Train Loss: 23842.5254, Test Loss: 2477152.5000\n",
            "Epoch: 055, Train Loss: 96899.3828, Test Loss: 2522516.7500\n",
            "Epoch: 056, Train Loss: 76261.7891, Test Loss: 2551680.7500\n",
            "Epoch: 057, Train Loss: 95818.3594, Test Loss: 2480573.2500\n",
            "Epoch: 058, Train Loss: 369058.5938, Test Loss: 2478307.0000\n",
            "Epoch: 059, Train Loss: 13919.1650, Test Loss: 2476328.7500\n",
            "Epoch: 060, Train Loss: 695714.8750, Test Loss: 2483387.0000\n",
            "Epoch: 061, Train Loss: 271630.7188, Test Loss: 2509348.5000\n",
            "Epoch: 062, Train Loss: 360063.7188, Test Loss: 2447323.5000\n",
            "Epoch: 063, Train Loss: 21380.2715, Test Loss: 2446944.0000\n",
            "Epoch: 064, Train Loss: 1123201.3750, Test Loss: 2502778.5000\n",
            "Epoch: 065, Train Loss: 92367.1484, Test Loss: 2448643.5000\n",
            "Epoch: 066, Train Loss: 3062728.0000, Test Loss: 2497838.5000\n",
            "Epoch: 067, Train Loss: 629654.5625, Test Loss: 2459583.5000\n",
            "Epoch: 068, Train Loss: 18210.5664, Test Loss: 2457128.5000\n",
            "Epoch: 069, Train Loss: 84312.1250, Test Loss: 2497758.0000\n",
            "Epoch: 070, Train Loss: 1298846.5000, Test Loss: 2537131.5000\n",
            "Epoch: 071, Train Loss: 298496.6875, Test Loss: 2541057.7500\n",
            "Epoch: 072, Train Loss: 357696.0312, Test Loss: 2503286.0000\n",
            "Epoch: 073, Train Loss: 19760.0293, Test Loss: 2475952.0000\n",
            "Epoch: 074, Train Loss: 310111.0938, Test Loss: 2526967.5000\n",
            "Epoch: 075, Train Loss: 11915.3340, Test Loss: 2495409.7500\n",
            "Epoch: 076, Train Loss: 124063.8828, Test Loss: 2497339.2500\n",
            "Epoch: 077, Train Loss: 13056.6992, Test Loss: 2523412.0000\n",
            "Epoch: 078, Train Loss: 120295.7500, Test Loss: 2498368.7500\n",
            "Epoch: 079, Train Loss: 1679136.8750, Test Loss: 2543723.0000\n",
            "Epoch: 080, Train Loss: 109784.8828, Test Loss: 2496254.0000\n",
            "Epoch: 081, Train Loss: 345989.2188, Test Loss: 2480403.7500\n",
            "Epoch: 082, Train Loss: 9536.2744, Test Loss: 2491660.5000\n",
            "Epoch: 083, Train Loss: 135510.0000, Test Loss: 2435691.5000\n",
            "Epoch: 084, Train Loss: 321713.8125, Test Loss: 2452408.7500\n",
            "Epoch: 085, Train Loss: 100313.8125, Test Loss: 2446907.2500\n",
            "Epoch: 086, Train Loss: 17344.4141, Test Loss: 2491933.0000\n",
            "Epoch: 087, Train Loss: 14659.4619, Test Loss: 2525361.7500\n",
            "Epoch: 088, Train Loss: 9669.4814, Test Loss: 2491863.5000\n",
            "Epoch: 089, Train Loss: 70257.1562, Test Loss: 2437755.0000\n",
            "Epoch: 090, Train Loss: 97893.8906, Test Loss: 2441911.5000\n",
            "Epoch: 091, Train Loss: 669246.3125, Test Loss: 2494723.5000\n",
            "Epoch: 092, Train Loss: 129880.7344, Test Loss: 2492347.0000\n",
            "Epoch: 093, Train Loss: 365262.7812, Test Loss: 2475300.7500\n",
            "Epoch: 094, Train Loss: 26720.2305, Test Loss: 2498680.0000\n",
            "Epoch: 095, Train Loss: 389502.6875, Test Loss: 2479739.2500\n",
            "Epoch: 096, Train Loss: 1346709.5000, Test Loss: 2471850.0000\n",
            "Epoch: 097, Train Loss: 23127.6055, Test Loss: 2482044.0000\n",
            "Epoch: 098, Train Loss: 230014.2344, Test Loss: 2480843.7500\n",
            "Epoch: 099, Train Loss: 112438.8281, Test Loss: 2493307.5000\n",
            "Epoch: 100, Train Loss: 123607.1094, Test Loss: 2557929.0000\n",
            "Epoch: 101, Train Loss: 9488.2646, Test Loss: 2480794.0000\n",
            "Epoch: 102, Train Loss: 543432.6250, Test Loss: 2499222.0000\n",
            "Epoch: 103, Train Loss: 580622.7500, Test Loss: 2471219.7500\n",
            "Epoch: 104, Train Loss: 120371.9062, Test Loss: 2486136.7500\n",
            "Epoch: 105, Train Loss: 2153679.7500, Test Loss: 2450816.5000\n",
            "Epoch: 106, Train Loss: 37399.5547, Test Loss: 2516206.7500\n",
            "Epoch: 107, Train Loss: 96930.6641, Test Loss: 2524831.5000\n",
            "Epoch: 108, Train Loss: 180077.8125, Test Loss: 2485490.0000\n",
            "Epoch: 109, Train Loss: 88156.6953, Test Loss: 2492685.7500\n",
            "Epoch: 110, Train Loss: 112938.6562, Test Loss: 2513596.0000\n",
            "Epoch: 111, Train Loss: 135001.0625, Test Loss: 2495587.0000\n",
            "Epoch: 112, Train Loss: 233009.2344, Test Loss: 2475807.7500\n",
            "Epoch: 113, Train Loss: 15187.8721, Test Loss: 2460304.0000\n",
            "Epoch: 114, Train Loss: 93697.2734, Test Loss: 2454182.0000\n",
            "Epoch: 115, Train Loss: 323212.3750, Test Loss: 2509491.7500\n",
            "Epoch: 116, Train Loss: 95244.8281, Test Loss: 2474754.5000\n",
            "Epoch: 117, Train Loss: 340009.0000, Test Loss: 2502465.2500\n",
            "Epoch: 118, Train Loss: 373215.1562, Test Loss: 2497036.7500\n",
            "Epoch: 119, Train Loss: 13947.7695, Test Loss: 2482996.7500\n",
            "Epoch: 120, Train Loss: 615482.8125, Test Loss: 2480538.5000\n",
            "Epoch: 121, Train Loss: 92046.5156, Test Loss: 2499809.2500\n",
            "Epoch: 122, Train Loss: 94085.0000, Test Loss: 2490320.2500\n",
            "Epoch: 123, Train Loss: 10357.6436, Test Loss: 2505337.5000\n",
            "Epoch: 124, Train Loss: 343091.9688, Test Loss: 2462840.7500\n",
            "Epoch: 125, Train Loss: 243155.1875, Test Loss: 2463899.0000\n",
            "Epoch: 126, Train Loss: 275554.9062, Test Loss: 2472800.0000\n",
            "Epoch: 127, Train Loss: 10054.1289, Test Loss: 2541034.5000\n",
            "Epoch: 128, Train Loss: 102075.7969, Test Loss: 2445632.0000\n",
            "Epoch: 129, Train Loss: 93175.0703, Test Loss: 2495432.2500\n",
            "Epoch: 130, Train Loss: 217363.1719, Test Loss: 2487878.7500\n",
            "Epoch: 131, Train Loss: 114160.3438, Test Loss: 2501347.5000\n",
            "Epoch: 132, Train Loss: 127258.6953, Test Loss: 2482160.0000\n",
            "Epoch: 133, Train Loss: 94565.8906, Test Loss: 2490161.2500\n",
            "Epoch: 134, Train Loss: 169641.3125, Test Loss: 2509876.7500\n",
            "Epoch: 135, Train Loss: 21375.0059, Test Loss: 2479070.2500\n",
            "Epoch: 136, Train Loss: 26772.5684, Test Loss: 2501915.7500\n",
            "Epoch: 137, Train Loss: 119989.8203, Test Loss: 2545977.0000\n",
            "Epoch: 138, Train Loss: 219352.2969, Test Loss: 2446467.7500\n",
            "Epoch: 139, Train Loss: 108412.8672, Test Loss: 2473259.7500\n",
            "Epoch: 140, Train Loss: 9546.3164, Test Loss: 2537476.2500\n",
            "Epoch: 141, Train Loss: 12530.2793, Test Loss: 2534057.5000\n",
            "Epoch: 142, Train Loss: 127639.9453, Test Loss: 2482506.0000\n",
            "Epoch: 143, Train Loss: 1141145.1250, Test Loss: 2482280.7500\n",
            "Epoch: 144, Train Loss: 126953.0781, Test Loss: 2483611.5000\n",
            "Epoch: 145, Train Loss: 319990.2500, Test Loss: 2506539.2500\n",
            "Epoch: 146, Train Loss: 569789.8125, Test Loss: 2440945.5000\n",
            "Epoch: 147, Train Loss: 724148.1875, Test Loss: 2488959.0000\n",
            "Epoch: 148, Train Loss: 5865773.0000, Test Loss: 2497682.0000\n",
            "Epoch: 149, Train Loss: 92660.6172, Test Loss: 2517527.7500\n",
            "Epoch: 150, Train Loss: 10308.3027, Test Loss: 2502199.5000\n",
            "Epoch: 151, Train Loss: 92101.1953, Test Loss: 2453326.7500\n",
            "Epoch: 152, Train Loss: 70041.3125, Test Loss: 2482632.7500\n",
            "Epoch: 153, Train Loss: 154224.0469, Test Loss: 2462660.2500\n",
            "Epoch: 154, Train Loss: 93850.5938, Test Loss: 2489103.2500\n",
            "Epoch: 155, Train Loss: 20134.7949, Test Loss: 2463943.5000\n",
            "Epoch: 156, Train Loss: 101254.1328, Test Loss: 2492723.5000\n",
            "Epoch: 157, Train Loss: 113058.1172, Test Loss: 2467765.5000\n",
            "Epoch: 158, Train Loss: 306689.2500, Test Loss: 2519318.0000\n",
            "Epoch: 159, Train Loss: 29327.8516, Test Loss: 2526482.5000\n",
            "Epoch: 160, Train Loss: 21224.7031, Test Loss: 2484000.7500\n",
            "Epoch: 161, Train Loss: 310093.5938, Test Loss: 2482620.0000\n",
            "Epoch: 162, Train Loss: 10084.4463, Test Loss: 2503889.2500\n",
            "Epoch: 163, Train Loss: 318742.2500, Test Loss: 2460503.7500\n",
            "Epoch: 164, Train Loss: 120362.1953, Test Loss: 2462427.5000\n",
            "Epoch: 165, Train Loss: 29723.6387, Test Loss: 2503762.7500\n",
            "Epoch: 166, Train Loss: 145838.1094, Test Loss: 2496215.5000\n",
            "Epoch: 167, Train Loss: 8579.6016, Test Loss: 2507602.0000\n",
            "Epoch: 168, Train Loss: 23952.2559, Test Loss: 2511518.7500\n",
            "Epoch: 169, Train Loss: 351711.4375, Test Loss: 2504595.5000\n",
            "Epoch: 170, Train Loss: 288145.9062, Test Loss: 2463433.7500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "REawBg-7Ksrz",
        "outputId": "673bdf82-1cac-40e1-c0ad-523ef6d6ecd4"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Data(x=[19, 11], edge_index=[2, 44], edge_attr=[44, 4], y=[1, 19], pos=[19, 3], idx=[1], name='gdb_78976', z=[19])"
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = dataset[0]\n",
        "data.to(device)\n",
        "predict, latent, outputs = model(data.edge_index, data.x)"
      ],
      "metadata": {
        "id": "z-hBrabxKzZL"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zELbelXiLIow",
        "outputId": "7464f4b4-bdff-4ea5-9e96-42f1e4054a1d"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([ 3.7165e+00,  7.9656e+01, -6.9316e+00,  3.3466e-01,  6.7760e+00,\n",
              "         1.1531e+03,  4.0558e+00, -1.1217e+04, -1.1217e+04, -1.1217e+04,\n",
              "        -1.1218e+04,  3.3927e+01, -8.2029e+01, -8.2510e+01, -8.2974e+01,\n",
              "        -7.6420e+01,  1.5397e+00,  1.3349e+00,  1.5131e+00], device='cuda:0',\n",
              "       grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data.y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpUkro1-LJ_N",
        "outputId": "faef2b31-f80e-41b8-f292-283fa76c8928"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.7379e+00,  7.5730e+01, -6.5770e+00,  2.2749e+00,  8.8519e+00,\n",
              "          8.6733e+02,  4.4423e+00, -1.0500e+04, -1.0500e+04, -1.0500e+04,\n",
              "         -1.0501e+04,  2.7433e+01, -8.2382e+01, -8.2946e+01, -8.3409e+01,\n",
              "         -7.6727e+01,  2.9823e+00,  1.9835e+00,  1.7279e+00]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    }
  ]
}